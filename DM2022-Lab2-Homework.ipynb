{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name:\n",
    "\n",
    "Student ID: 108048110\n",
    "\n",
    "GitHub ID: MeganKuo0704\n",
    "\n",
    "Kaggle name:\n",
    "\n",
    "Kaggle private scoreboard snapshot:\n",
    "\n",
    "[Snapshot](img/pic0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: __This part is worth 30% of your grade.__ Do the **take home** exercises in the [DM2022-Lab2-master Repo](https://github.com/keziatamus/DM2022-Lab2-Master). You may need to copy some cells from the Lab notebook to this notebook. \n",
    "\n",
    "\n",
    "2. Second: __This part is worth 30% of your grade.__ Participate in the in-class [Kaggle Competition](https://www.kaggle.com/competitions/dm2022-isa5810-lab2-homework) regarding Emotion Recognition on Twitter. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20% of the 30% available for this section.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (60-x)/6 + 20 points, where x is your ranking in the leaderboard (ie. If you rank 3rd your score will be (60-3)/6 + 20 = 29.5% out of 30%)   \n",
    "    Submit your last submission __BEFORE the deadline (Dec. 15th 11:59 pm, Tuesday)__. Make sure to take a screenshot of your position at the end of the competition and store it as '''pic0.png''' under the **img** folder of this repository and rerun the cell **Student Information**.\n",
    "    \n",
    "\n",
    "3. Third: __This part is worth 30% of your grade.__ A report of your work developping the model for the competition (You can use code and comment it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained. \n",
    "\n",
    "\n",
    "4. Fourth: __This part is worth 10% of your grade.__ It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook** and **add minimal comments where needed**.\n",
    "\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding e-learn assignment.\n",
    "\n",
    "Make sure to commit and save your changes to your repository __BEFORE the deadline (Dec. 18th 11:59 pm, Friday)__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin Assignment Here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import os\n",
    "import re\n",
    "from itertools import chain\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import CSVLogger\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = pd.read_csv('C:\\\\Users\\\\megan\\\\Desktop\\\\DSPMT\\\\DS\\\\Data Mining\\\\Lab2\\\\dataset\\\\data_identification.csv')\n",
    "emo = pd.read_csv('C:\\\\Users\\\\megan\\\\Desktop\\\\DSPMT\\DS\\\\Data Mining\\Lab2\\dataset\\\\emotion.csv')\n",
    "tweets = pd.read_json('C:\\\\Users\\\\megan\\\\Desktop\\\\DSPMT\\\\DS\\\\Data Mining\\\\Lab2\\\\dataset\\\\tweets_DM.json', lines=True)\n",
    "\n",
    "train_id = id[id.identification=='train']\n",
    "train_emo = emo[emo.tweet_id.isin(train_id.tweet_id)]\n",
    "tweets['_index'] = [tweets._source[i]['tweet']['tweet_id'] for i in range(0, tweets.shape[0])]\n",
    "tweets['hashtags'] = [tweets._source[i]['tweet']['hashtags'] for i in range(0, tweets.shape[0])]\n",
    "tweets['text'] = [tweets._source[i]['tweet']['text'] for i in range(0, tweets.shape[0])]\n",
    "\n",
    "tweets['year'] = tweets._crawldate.apply(lambda x: x.split()[0].split('-')[0])\n",
    "tweets['month'] = tweets._crawldate.apply(lambda x: x.split()[0].split('-')[1])\n",
    "tweets['day'] = tweets._crawldate.apply(lambda x: x.split()[0].split('-')[2])\n",
    "tweets['hour'] = tweets._crawldate.apply(lambda x: x.split()[1].split(':')[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets._crawldate = pd.to_datetime(tweets.iloc[:, 7:])\n",
    "tweets._crawldate -= min(tweets._crawldate)\n",
    "\n",
    "tweets.drop(['_source', '_type', 'year', 'month', 'day', 'hour'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.to_pickle('tweets_clean.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_pickle('tweets_clean.pkl')\n",
    "emo = pd.read_csv('C:\\\\Users\\\\megan\\\\Desktop\\\\DSPMT\\DS\\\\Data Mining\\Lab2\\dataset\\\\emotion.csv')\n",
    "id = pd.read_csv('C:\\\\Users\\\\megan\\\\Desktop\\\\DSPMT\\\\DS\\\\Data Mining\\\\Lab2\\\\dataset\\\\data_identification.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id = id[id.identification=='train']\n",
    "train_emo = emo[emo.tweet_id.isin(train_id.tweet_id)]\n",
    "\n",
    "test_id = id[id.identification=='test']\n",
    "test_df = tweets[tweets['_index'].isin(test_id.tweet_id)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_emo.columns = ['_index', 'emo']\n",
    "train_df = pd.merge(tweets, train_emo, how='inner', on='_index')\n",
    "\n",
    "test_df = train_df.emo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_score</th>\n",
       "      <th>_index</th>\n",
       "      <th>_crawldate</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text</th>\n",
       "      <th>hour</th>\n",
       "      <th>emo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>391</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>142 days 10:00:00</td>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>11</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>433</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>392 days 03:00:00</td>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>04</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>376</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>388 days 22:00:00</td>\n",
       "      <td>[]</td>\n",
       "      <td>Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ &lt;LH&gt;</td>\n",
       "      <td>23</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>120</td>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>161 days 03:00:00</td>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>04</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1021</td>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>229 days 01:00:00</td>\n",
       "      <td>[]</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>02</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455558</th>\n",
       "      <td>94</td>\n",
       "      <td>0x321566</td>\n",
       "      <td>725 days 01:00:00</td>\n",
       "      <td>[NoWonder, Happy]</td>\n",
       "      <td>I'm SO HAPPY!!! #NoWonder the name of this sho...</td>\n",
       "      <td>02</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455559</th>\n",
       "      <td>627</td>\n",
       "      <td>0x38959e</td>\n",
       "      <td>90 days 07:00:00</td>\n",
       "      <td>[]</td>\n",
       "      <td>In every circumtance I'd like to be thankful t...</td>\n",
       "      <td>08</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455560</th>\n",
       "      <td>274</td>\n",
       "      <td>0x2cbca6</td>\n",
       "      <td>686 days 22:00:00</td>\n",
       "      <td>[blessyou]</td>\n",
       "      <td>there's currently two girls walking around the...</td>\n",
       "      <td>23</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455561</th>\n",
       "      <td>840</td>\n",
       "      <td>0x24faed</td>\n",
       "      <td>610 days 13:00:00</td>\n",
       "      <td>[]</td>\n",
       "      <td>Ah, corporate life, where you can date &lt;LH&gt; us...</td>\n",
       "      <td>14</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455562</th>\n",
       "      <td>360</td>\n",
       "      <td>0x34be8c</td>\n",
       "      <td>685 days 00:00:00</td>\n",
       "      <td>[Sundayvibes]</td>\n",
       "      <td>Blessed to be living #Sundayvibes &lt;LH&gt;</td>\n",
       "      <td>01</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1455563 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         _score    _index        _crawldate                       hashtags  \\\n",
       "0           391  0x376b20 142 days 10:00:00                     [Snapchat]   \n",
       "1           433  0x2d5350 392 days 03:00:00  [freepress, TrumpLegacy, CNN]   \n",
       "2           376  0x1cd5b0 388 days 22:00:00                             []   \n",
       "3           120  0x1d755c 161 days 03:00:00      [authentic, LaughOutLoud]   \n",
       "4          1021  0x2c91a8 229 days 01:00:00                             []   \n",
       "...         ...       ...               ...                            ...   \n",
       "1455558      94  0x321566 725 days 01:00:00              [NoWonder, Happy]   \n",
       "1455559     627  0x38959e  90 days 07:00:00                             []   \n",
       "1455560     274  0x2cbca6 686 days 22:00:00                     [blessyou]   \n",
       "1455561     840  0x24faed 610 days 13:00:00                             []   \n",
       "1455562     360  0x34be8c 685 days 00:00:00                  [Sundayvibes]   \n",
       "\n",
       "                                                      text hour           emo  \n",
       "0        People who post \"add me on #Snapchat\" must be ...   11  anticipation  \n",
       "1        @brianklaas As we see, Trump is dangerous to #...   04       sadness  \n",
       "2                      Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <LH>   23          fear  \n",
       "3        @RISKshow @TheKevinAllison Thx for the BEST TI...   04           joy  \n",
       "4             Still waiting on those supplies Liscus. <LH>   02  anticipation  \n",
       "...                                                    ...  ...           ...  \n",
       "1455558  I'm SO HAPPY!!! #NoWonder the name of this sho...   02           joy  \n",
       "1455559  In every circumtance I'd like to be thankful t...   08           joy  \n",
       "1455560  there's currently two girls walking around the...   23           joy  \n",
       "1455561  Ah, corporate life, where you can date <LH> us...   14           joy  \n",
       "1455562             Blessed to be living #Sundayvibes <LH>   01           joy  \n",
       "\n",
       "[1455563 rows x 7 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def preprocess_text(sen):\n",
    "    # Removing html tags\n",
    "    sentence = TAG_RE.sub('', sen)\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "    return sentence\n",
    "\n",
    "def tokenizer(data):\n",
    "    tokenize_model = CountVectorizer(max_features = 500, tokenizer=nltk.word_tokenize)\n",
    "    tokenize_model.fit(data['text'])\n",
    "    token_data = tokenize_model.transform(data['text'])\n",
    "\n",
    "    return token_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'People who post add me on Snapchat must be dehydrated Cuz man that '"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_text(train_df.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = train_df.text.apply(lambda x: preprocess_text(x))\n",
    "test_tokens = test_df.text.apply(lambda x: preprocess_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\megan\\python\\lib\\site-packages\\pandas\\core\\generic.py:5494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "train_df.text = train_tokens\n",
    "test_df.text = test_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(train_df._crawldate.to_numpy().reshape(train_df.shape[0], 1))\n",
    "day_range = scaler.transform(train_df._crawldate.to_numpy().reshape(train_df.shape[0], 1))\n",
    "train_df._crawldate = day_range\n",
    "\n",
    "scaler.fit(test_df._crawldate.to_numpy().reshape(test_df.shape[0], 1))\n",
    "day_range = scaler.transform(test_df._crawldate.to_numpy().reshape(test_df.shape[0], 1))\n",
    "test_df._crawldate = day_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(train_df._score.to_numpy().reshape(train_df.shape[0], 1))\n",
    "score_range = scaler.transform(train_df._score.to_numpy().reshape(train_df.shape[0], 1))\n",
    "train_df._score = score_range\n",
    "\n",
    "scaler.fit(test_df._score.to_numpy().reshape(test_df.shape[0], 1))\n",
    "score_range = scaler.transform(test_df._score.to_numpy().reshape(test_df.shape[0], 1))\n",
    "test_df._score = score_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_pickle('train_df.pkl')\n",
    "test_df.to_pickle('test_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle('train_df.pkl')\n",
    "test_df = pd.read_pickle('test_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.sample(frac=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(727782, 7)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train_test_split(train_df, test_size = 0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_train.emo\n",
    "y_test = y_train.emo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\megan\\python\\lib\\site-packages\\pandas\\core\\frame.py:4308: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "X_train.drop(['emo', '_index', 'hashtags'], inplace=True, axis=1)\n",
    "y_train.drop(['emo', '_index', 'hashtags'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\megan\\python\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# build analyzers (bag-of-words)\n",
    "BOW_500 = CountVectorizer(max_features=500, tokenizer=nltk.word_tokenize) \n",
    "\n",
    "# apply analyzer to training data\n",
    "BOW_500.fit(train_df['text'])\n",
    "\n",
    "train_features = BOW_500.transform(X_train.text)\n",
    "val_features = BOW_500.transform(y_train.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_features = val_features.astype('int32')\n",
    "train_features = train_features.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\megan\\python\\lib\\site-packages\\pandas\\core\\generic.py:5494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "X_train._crawldate = X_train._crawldate.astype('float32')\n",
    "X_train._score = X_train._score.astype('float32')\n",
    "y_train._crawldate = y_train._crawldate.astype('float32')\n",
    "y_train._score = y_train._score.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.append(X_train._crawldate.to_numpy().reshape(X_train.shape[0], 1), np.array(X_train._score).reshape(X_train.shape[0], 1), axis=1)\n",
    "train_features = np.append(train_features.toarray(), tmp, axis=1).astype('float32')\n",
    "tmp = np.append(y_train._crawldate.to_numpy().reshape(y_train.shape[0], 1), np.array(y_train._score).reshape(y_train.shape[0], 1), axis=1)\n",
    "val_features = np.append(val_features.toarray(), tmp, axis=1).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((509447, 502), (509447,), (218335, 502), (218335,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape, X_test.shape, val_features.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_features = BOW_500.transform(test_df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.append(np.array(test_df._crawldate).reshape(test_df.shape[0], 1), np.array(test_df._score).reshape(test_df.shape[0], 1), axis=1)\n",
    "target_features = np.append(target_features.toarray(), tmp, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check label:  ['anger' 'anticipation' 'disgust' 'fear' 'joy' 'sadness' 'surprise'\n",
      " 'trust']\n",
      "\n",
      "## Before convert\n",
      "test_df[0:7]:\n",
      " 1061508            fear\n",
      "26052               joy\n",
      "133593              joy\n",
      "309260              joy\n",
      "232670              joy\n",
      "952244            trust\n",
      "814660     anticipation\n",
      "Name: emo, dtype: object\n",
      "\n",
      "test_df.shape:  (727782,)\n",
      "\n",
      "\n",
      "## After convert\n",
      "test_df[0:7]:\n",
      " 1061508            fear\n",
      "26052               joy\n",
      "133593              joy\n",
      "309260              joy\n",
      "232670              joy\n",
      "952244            trust\n",
      "814660     anticipation\n",
      "Name: emo, dtype: object\n",
      "\n",
      "test_df.shape:  (727782,)\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(train_df.emo)  # learn categorical labels\n",
    "\n",
    "print('check label: ', label_encoder.classes_)\n",
    "print('\\n## Before convert')\n",
    "print('test_df[0:7]:\\n', train_df.emo[0:7])\n",
    "print('\\ntest_df.shape: ', train_df.emo.shape)\n",
    "\n",
    "def label_encode(le, labels):\n",
    "    enc = le.transform(labels)\n",
    "    return keras.utils.np_utils.to_categorical(enc)\n",
    "\n",
    "def label_decode(le, one_hot_label):\n",
    "    dec = np.argmax(one_hot_label, axis=1)\n",
    "    return le.inverse_transform(dec)\n",
    "\n",
    "x_test = label_encode(label_encoder, X_test)\n",
    "val_test = label_encode(label_encoder, y_test)\n",
    "\n",
    "\n",
    "print('\\n\\n## After convert')\n",
    "print('test_df[0:7]:\\n', train_df.emo[0:7])\n",
    "print('\\ntest_df.shape: ', train_df.emo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape:  502\n",
      "output_shape:  8\n"
     ]
    }
   ],
   "source": [
    "# I/O check\n",
    "input_shape = train_features.shape[1]\n",
    "print('input_shape: ', input_shape)\n",
    "\n",
    "output_shape = len(label_encoder.classes_)\n",
    "print('output_shape: ', output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, Bidirectional\n",
    "from keras.layers import Embedding, GRU, SimpleRNN, LSTM\n",
    "from keras.layers import ReLU, Softmax\n",
    "from keras.constraints import maxnorm\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_train_features = train_features.reshape(, train_features.shape[0], train_features.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101889, 1202)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_val_features = val_features.reshape(1, val_features.shape[0], val_features.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, None, 500)         601000    \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, None, 256)        644096    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " seq_self_attention_1 (SeqSe  (None, None, 256)        16449     \n",
      " lfAttention)                                                    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, None, 8)           2056      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,263,601\n",
      "Trainable params: 1,263,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras_self_attention import SeqSelfAttention\n",
    "\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Embedding(input_dim=input_shape,\n",
    "                                 output_dim=500,\n",
    "                                 mask_zero=True))\n",
    "\n",
    "model.add(keras.layers.Bidirectional(keras.layers.LSTM(units=128,\n",
    "                                                       return_sequences=True)))\n",
    "model.add(SeqSelfAttention(attention_activation='sigmoid'))\n",
    "model.add(keras.layers.Dense(units=8))\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['categorical_accuracy'],\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, None, 64)          32128     \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, None, 128)        66048     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 64)               41216     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense (Dense)               (None, 8)                 520       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 139,912\n",
      "Trainable params: 139,912\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, None, 64)          32128     \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, None, 128)        66048     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 64)               41216     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense (Dense)               (None, 8)                 520       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 139,912\n",
      "Trainable params: 139,912\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# input layer\n",
    "# input layer\n",
    "model = keras.Sequential()\n",
    "\n",
    "model.add(Embedding(input_dim=input_shape, output_dim = 64, mask_zero=True))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(32, return_sequences=False)))\n",
    "model.add(Dense(8))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# loss function & optimizer\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# show model construction\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Attention, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 1202)]            0         \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 512)               615936    \n",
      "                                                                 \n",
      " re_lu (ReLU)                (None, 512)               0         \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 512)               262656    \n",
      "                                                                 \n",
      " re_lu_1 (ReLU)              (None, 512)               0         \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " re_lu_2 (ReLU)              (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " re_lu_3 (ReLU)              (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " re_lu_4 (ReLU)              (None, 128)               0         \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 8)                 1032      \n",
      "                                                                 \n",
      " softmax (Softmax)           (None, 8)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,109,640\n",
      "Trainable params: 1,109,640\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "epochs = 40 # total num of iterations\n",
    "batch_size = 64 # pass 32 samples to the network afrom keras.models import Model\n",
    "\n",
    "# input layer\n",
    "model_input = Input(shape=(input_shape, ))  # 500\n",
    "X = model_input\n",
    "\n",
    "# 1st hidden layer\n",
    "X_W1 = Dense(units=512)(X)  # 64\n",
    "H1 = ReLU()(X_W1)\n",
    "D1 = Dropout(0.3)(H1)\n",
    "\n",
    "# 2nd hidden layer\n",
    "H1_W2 = Dense(units=512)(D1)  # 64\n",
    "H2 = ReLU()(H1_W2)\n",
    "D2 = Dropout(0.3)(H2)\n",
    "\n",
    "# 3nd hidden layer\n",
    "H1_W3 = Dense(units=256)(D2)  # 64\n",
    "H3 = ReLU()(H1_W3)\n",
    "D3 = Dropout(0.3)(H3)\n",
    "\n",
    "# 4nd hidden layer\n",
    "H1_W4 = Dense(units=256)(D3)  # 64\n",
    "H4 = ReLU()(H1_W4)\n",
    "D4 = Dropout(0.3)(H4)\n",
    "\n",
    "# 5nd hidden layer\n",
    "H1_W5 = Dense(units=128)(D4)  # 64\n",
    "H5 = ReLU()(H1_W5)\n",
    "\n",
    "# output layer\n",
    "H2_W1 = Dense(units=output_shape)(H5)  # 4\n",
    "H11 = Softmax()(H2_W1)\n",
    "\n",
    "model_output = H11\n",
    "\n",
    "# create model\n",
    "model = Model(inputs=[model_input], outputs=[model_output])\n",
    "\n",
    "# loss function & optimizer\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# show model construction\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding_6 (Embedding)     (None, None, 64)          76928     \n",
      "                                                                 \n",
      " seq_self_attention_6 (SeqSe  (None, None, 64)         4161      \n",
      " lfAttention)                                                    \n",
      "                                                                 \n",
      " simple_rnn_4 (SimpleRNN)    (None, 128)               24704     \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 8)                 520       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 114,569\n",
      "Trainable params: 114,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "epochs = 10 # total num of iterations\n",
    "batch_size = 64 # pass 32 samples to the network afrom keras.models import Model\n",
    "\n",
    "model_input = Input(shape=(None, ))  # 500\n",
    "X = model_input\n",
    "\n",
    "# input layer\n",
    "E1 = Embedding(input_dim=input_shape, output_dim = 64, mask_zero=True)(X)\n",
    "att = SeqSelfAttention(attention_activation='sigmoid')(E1)\n",
    "RNN_layer = SimpleRNN(64, return_sequences=False, activation='tanh')(att)\n",
    "# output layer\n",
    "O1 = Dense(units=output_shape, activation='softmax')(RNN_layer)  # 4\n",
    "\n",
    "model_output = O1\n",
    "\n",
    "model = Model(inputs=[model_input], outputs=[model_output])\n",
    "# loss function & optimizer\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# show model construction\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_features.toarray()\n",
    "val_features = val_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_features.astype('float32')\n",
    "val_features = val_features.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1018894, 500)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1018894, 8)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_x_test = x_test.reshape(1, x_test.shape[0], x_test.shape[1])\n",
    "lstm_val_test = val_test.reshape(1, val_test.shape[0], val_test.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "16982/16982 [==============================] - 13816s 813ms/step - loss: 7.2408 - accuracy: 0.3539 - val_loss: 7.2419 - val_accuracy: 0.3558\n",
      "Epoch 2/5\n",
      " 8336/16982 [=============>................] - ETA: 5:43:32 - loss: 7.2479 - accuracy: 0.3543"
     ]
    }
   ],
   "source": [
    "# csv_logger = CSVLogger('logs/training_log.csv')\n",
    "\n",
    "# training setting\n",
    "epochs = 5 # total num of iterations\n",
    "batch_size = 30 # pass 32 samples to the network at a time\n",
    "\n",
    "# training!\n",
    "history = model.fit(train_features, x_test, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    validation_data = (val_features, val_test))\n",
    "print('training finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(218335, 8)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_target_features = target_features.reshape(1, target_features.shape[0], target_features.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "## predict\n",
    "target_pred = model.predict(target_features, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['anticipation', 'anticipation', 'joy', 'joy', 'joy'], dtype=object)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_pred = label_decode(label_encoder, target_pred)\n",
    "target_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(np.append(test_df['_index'].to_numpy().reshape(test_df.shape[0], 1), target_pred.reshape(test_df.shape[0], 1), axis=1), columns=['id', 'emotion'])\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "17b6f5ac0cdde9bb0319db34b1228fea907021b8a20d2a4fdce6ac8225297b92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
